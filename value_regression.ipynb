{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from normalizers import Normalization, NormalizationInverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "        self.open_dataset()\n",
    "        self.load_part()\n",
    "        \n",
    "    def open_dataset(self):\n",
    "        self.f = open(self.fn, 'rb')\n",
    "        \n",
    "    def load_part(self):\n",
    "        self.access_counter = 0\n",
    "        try:\n",
    "            self.part = pickle.load(self.f)\n",
    "        except EOFError:\n",
    "            self.f.close()\n",
    "            self.open_dataset()\n",
    "            self.load_part()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        self.access_counter += 1\n",
    "        if self.access_counter > len(self):\n",
    "            self.load_part()\n",
    "        return list(map(lambda x: x.astype(np.float32), self.part[i]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.part)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.f.close()\n",
    "\n",
    "class ValueFunction(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_size, params_size):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size + params_size, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs, params):\n",
    "        x = torch.cat([obs, params], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class QFunction(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_size, params_size, action_size):\n",
    "        super(QFunction, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size + params_size + action_size, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs, params, action):\n",
    "        x = torch.cat([obs, params, action], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Policy(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_size, params_size, action_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size + params_size, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, 400),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(400, action_size),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs, params):\n",
    "        x = torch.cat([obs, params], dim=1)\n",
    "        return 2 * self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dataset = FileDataset('value_dataset.pkl')\n",
    "dataloader = DataLoader(v_dataset, batch_size=128, shuffle=True)\n",
    "obs, params, values = next(iter(dataloader))\n",
    "value_norm = Normalization(1)\n",
    "value_norm.std = torch.FloatTensor([values.std()]).float()\n",
    "\n",
    "obs_norm = Normalization(obs.shape[1])\n",
    "obs_norm.mean = obs.mean(dim=0)\n",
    "obs_norm.std = obs.std(dim=0)\n",
    "\n",
    "params_norm = Normalization(params.shape[1])\n",
    "params_norm.mean = params.mean(dim=0)\n",
    "params_norm.std = params.std(dim=0)\n",
    "\n",
    "V = ValueFunction(obs.shape[1], params.shape[1])\n",
    "\n",
    "obs_norm.cuda()\n",
    "params_norm.cuda()\n",
    "value_norm.cuda()\n",
    "V.train()\n",
    "V.cuda()\n",
    "optim = torch.optim.Adam(V.parameters(), weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000\n",
      "Epoch 100/1000\n",
      "Epoch 200/1000\n",
      "Epoch 300/1000\n",
      "Epoch 400/1000\n",
      "Epoch 500/1000\n",
      "Epoch 600/1000\n",
      "Epoch 700/1000\n",
      "Epoch 800/1000\n",
      "Epoch 900/1000\n",
      "Epoch 1000/1000\n",
      "Epoch 1100/1000\n",
      "Epoch 1200/1000\n",
      "Epoch 1300/1000\n",
      "Epoch 1400/1000\n",
      "Epoch 1500/1000\n",
      "Epoch 1600/1000\n",
      "Epoch 1700/1000\n",
      "Epoch 1800/1000\n",
      "Epoch 1900/1000\n",
      "Epoch 2000/1000\n",
      "Epoch 2100/1000\n",
      "Epoch 2200/1000\n",
      "Epoch 2300/1000\n",
      "Epoch 2400/1000\n",
      "Epoch 2500/1000\n",
      "Epoch 2600/1000\n",
      "Epoch 2700/1000\n",
      "Epoch 2800/1000\n",
      "Epoch 2900/1000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3000):\n",
    "    for obs, params, values in dataloader:\n",
    "        O = obs_norm(Variable(obs.cuda()))\n",
    "        Z = params_norm(Variable(params.cuda()))\n",
    "        V_ = value_norm(Variable(values.cuda()))\n",
    "        loss = torch.nn.functional.mse_loss(V(O, Z), V_)\n",
    "        V.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}/{}'.format(epoch, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.13309032, -0.08523302),\n",
       " (-3.270193, -3.1242554),\n",
       " (-2.9093504, -2.866458),\n",
       " (-1.2853628, -1.376354),\n",
       " (-0.4896473, -0.44846517),\n",
       " (-0.13522452, -0.084901124),\n",
       " (-0.32849696, -0.38830024),\n",
       " (-0.27185962, -0.21128078),\n",
       " (-0.4865954, -0.44909126),\n",
       " (-1.048843, -1.0561224)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.eval()\n",
    "obs, params, values = next(iter(dataloader))\n",
    "O = obs_norm(Variable(obs.cuda()))\n",
    "Z = params_norm(Variable(params.cuda()))\n",
    "V_ = value_norm(Variable(values.cuda()))\n",
    "list(zip(V(O, Z).cpu().data.numpy().flatten(), V_.cpu().data.numpy().flatten()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dataset = FileDataset('q_dataset.pkl')\n",
    "q_dataloader = DataLoader(q_dataset, batch_size=128, shuffle=True)\n",
    "obs, obs_, params, actions, rewards = next(iter(q_dataloader))\n",
    "Q = QFunction(obs.shape[1], params.shape[1], actions.shape[1]).cuda()\n",
    "q_optim = torch.optim.Adam(Q.parameters(), weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000\n",
      "Epoch 100/1000\n",
      "Epoch 200/1000\n",
      "Epoch 300/1000\n",
      "Epoch 400/1000\n",
      "Epoch 500/1000\n",
      "Epoch 600/1000\n",
      "Epoch 700/1000\n",
      "Epoch 800/1000\n",
      "Epoch 900/1000\n",
      "Epoch 1000/1000\n",
      "Epoch 1100/1000\n",
      "Epoch 1200/1000\n",
      "Epoch 1300/1000\n",
      "Epoch 1400/1000\n",
      "Epoch 1500/1000\n",
      "Epoch 1600/1000\n",
      "Epoch 1700/1000\n",
      "Epoch 1800/1000\n",
      "Epoch 1900/1000\n",
      "Epoch 2000/1000\n",
      "Epoch 2100/1000\n",
      "Epoch 2200/1000\n",
      "Epoch 2300/1000\n",
      "Epoch 2400/1000\n",
      "Epoch 2500/1000\n",
      "Epoch 2600/1000\n",
      "Epoch 2700/1000\n",
      "Epoch 2800/1000\n",
      "Epoch 2900/1000\n"
     ]
    }
   ],
   "source": [
    "V.eval()\n",
    "Q.train()\n",
    "gamma = 0.99\n",
    "for epoch in range(3000):\n",
    "    for obs, obs_, params, actions, rewards in q_dataloader:\n",
    "        O = obs_norm(Variable(obs.cuda()))\n",
    "        O_ = obs_norm(Variable(obs_.cuda()))\n",
    "        Z = params_norm(Variable(params.cuda()))\n",
    "        U = Variable(actions.cuda())\n",
    "        R = value_norm(Variable(rewards.cuda()))\n",
    "        Q.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(Q(O, Z, U), R + gamma * V(O_, Z))\n",
    "        loss.backward()\n",
    "        q_optim.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {}/{}'.format(epoch, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.25833604, -0.2650023),\n",
       " (-0.22623555, -0.20397973),\n",
       " (-0.54944247, -0.5527098),\n",
       " (-0.74925727, -0.7466393),\n",
       " (-1.4515071, -1.4175148),\n",
       " (-0.08390489, -0.08829112),\n",
       " (-0.31667238, -0.31854355),\n",
       " (-1.8858371, -1.9164882),\n",
       " (-1.2532898, -1.1960375),\n",
       " (-1.0725285, -1.0942819),\n",
       " (-0.45137706, -0.4738177),\n",
       " (-0.36405244, -0.3667941),\n",
       " (-0.57929295, -0.5886372),\n",
       " (-1.1121478, -1.0097613),\n",
       " (-0.31569174, -0.30413264),\n",
       " (-0.26729536, -0.27853101),\n",
       " (-1.0838315, -1.0258398),\n",
       " (-2.1214082, -2.0699651),\n",
       " (-0.51481646, -0.5265689),\n",
       " (-0.38244626, -0.3922896),\n",
       " (-0.34314126, -0.3866646),\n",
       " (-0.46637166, -0.49444035),\n",
       " (-0.14358595, -0.1422805),\n",
       " (-0.87791204, -0.8644337),\n",
       " (-4.535679, -4.4966125),\n",
       " (-0.7758873, -0.8036545),\n",
       " (-0.9946222, -0.95786977),\n",
       " (-1.1254731, -1.1369884),\n",
       " (-0.6363138, -0.6421214),\n",
       " (-1.8671442, -1.8588238),\n",
       " (-0.06676618, -0.053315528),\n",
       " (-0.7876857, -0.9039819)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, obs_, params, actions, rewards = next(iter(q_dataloader))\n",
    "list(zip((R + gamma * V(O_, Z)).cpu().data.numpy().flatten(),\n",
    "Q(O, Z, U).cpu().data.numpy().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Policy(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=400, out_features=400, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=400, out_features=7, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, obs_, params, actions, rewards = next(iter(q_dataloader))\n",
    "policy = Policy(obs.shape[1], params.shape[1], actions.shape[1])\n",
    "policy_opt = torch.optim.Adam(policy.parameters(), weight_decay=1e-3)\n",
    "policy.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.47389221191406\n",
      "0.3457089066505432\n",
      "Epoch 0/1000\n",
      "24.385013580322266\n",
      "0.07930387556552887\n",
      "Epoch 50/1000\n",
      "29.581445693969727\n",
      "0.10102365911006927\n",
      "Epoch 100/1000\n",
      "34.42053985595703\n",
      "0.07728796452283859\n",
      "Epoch 150/1000\n",
      "33.95734786987305\n",
      "0.0995962992310524\n",
      "Epoch 200/1000\n",
      "27.44752311706543\n",
      "0.10134600102901459\n",
      "Epoch 250/1000\n",
      "26.505075454711914\n",
      "0.10819465667009354\n",
      "Epoch 300/1000\n",
      "33.285118103027344\n",
      "0.08796384930610657\n",
      "Epoch 350/1000\n",
      "27.331392288208008\n",
      "0.08469034731388092\n",
      "Epoch 400/1000\n",
      "31.13029670715332\n",
      "0.11473171412944794\n",
      "Epoch 450/1000\n",
      "32.12821578979492\n",
      "0.08628184348344803\n",
      "Epoch 500/1000\n",
      "35.721092224121094\n",
      "0.11199020594358444\n",
      "Epoch 550/1000\n",
      "22.140432357788086\n",
      "0.11062586307525635\n",
      "Epoch 600/1000\n",
      "33.79481887817383\n",
      "0.1048588678240776\n",
      "Epoch 650/1000\n",
      "28.77617073059082\n",
      "0.11680149286985397\n",
      "Epoch 700/1000\n",
      "31.310190200805664\n",
      "0.12326891720294952\n",
      "Epoch 750/1000\n",
      "32.23411178588867\n",
      "0.09736748784780502\n",
      "Epoch 800/1000\n",
      "30.138240814208984\n",
      "0.11759686470031738\n",
      "Epoch 850/1000\n",
      "28.250041961669922\n",
      "0.11218862980604172\n",
      "Epoch 900/1000\n",
      "29.856840133666992\n",
      "0.09610885381698608\n",
      "Epoch 950/1000\n",
      "33.961185455322266\n",
      "0.12177108973264694\n",
      "Epoch 1000/1000\n",
      "24.831411361694336\n",
      "0.11446361243724823\n",
      "Epoch 1050/1000\n",
      "26.270715713500977\n",
      "0.0850490853190422\n",
      "Epoch 1100/1000\n",
      "34.70600128173828\n",
      "0.10154871642589569\n",
      "Epoch 1150/1000\n",
      "34.31504821777344\n",
      "0.12068843096494675\n",
      "Epoch 1200/1000\n",
      "26.11823081970215\n",
      "0.08957188576459885\n",
      "Epoch 1250/1000\n",
      "27.80697250366211\n",
      "0.11097191274166107\n",
      "Epoch 1300/1000\n",
      "32.28694152832031\n",
      "0.1017874926328659\n",
      "Epoch 1350/1000\n",
      "29.018218994140625\n",
      "0.07782966643571854\n",
      "Epoch 1400/1000\n",
      "26.75177574157715\n",
      "0.10703389346599579\n",
      "Epoch 1450/1000\n",
      "29.945453643798828\n",
      "0.09573063254356384\n",
      "Epoch 1500/1000\n",
      "27.177488327026367\n",
      "0.11129690706729889\n",
      "Epoch 1550/1000\n",
      "37.11507797241211\n",
      "0.12093314528465271\n",
      "Epoch 1600/1000\n",
      "29.374149322509766\n",
      "0.1063193529844284\n",
      "Epoch 1650/1000\n",
      "23.776233673095703\n",
      "0.11087264865636826\n",
      "Epoch 1700/1000\n",
      "33.34661865234375\n",
      "0.12801721692085266\n",
      "Epoch 1750/1000\n",
      "29.010025024414062\n",
      "0.10666706413030624\n",
      "Epoch 1800/1000\n",
      "29.748716354370117\n",
      "0.08584307134151459\n",
      "Epoch 1850/1000\n",
      "32.04071807861328\n",
      "0.11726752668619156\n",
      "Epoch 1900/1000\n",
      "18.090255737304688\n",
      "0.09971661120653152\n",
      "Epoch 1950/1000\n",
      "32.47077560424805\n",
      "0.09290064871311188\n",
      "Epoch 2000/1000\n",
      "27.229087829589844\n",
      "0.09612292796373367\n",
      "Epoch 2050/1000\n",
      "31.479013442993164\n",
      "0.09260443598031998\n",
      "Epoch 2100/1000\n",
      "29.457735061645508\n",
      "0.10642338544130325\n",
      "Epoch 2150/1000\n",
      "38.56834411621094\n",
      "0.10379601269960403\n",
      "Epoch 2200/1000\n",
      "36.296363830566406\n",
      "0.11104205250740051\n",
      "Epoch 2250/1000\n",
      "22.098474502563477\n",
      "0.10287228971719742\n",
      "Epoch 2300/1000\n",
      "35.81013488769531\n",
      "0.10008186101913452\n",
      "Epoch 2350/1000\n",
      "28.32880401611328\n",
      "0.10708847641944885\n",
      "Epoch 2400/1000\n",
      "28.723230361938477\n",
      "0.09173239022493362\n",
      "Epoch 2450/1000\n",
      "26.034555435180664\n",
      "0.11997133493423462\n",
      "Epoch 2500/1000\n",
      "44.31048583984375\n",
      "0.1138901337981224\n",
      "Epoch 2550/1000\n",
      "41.6018180847168\n",
      "0.1127631738781929\n",
      "Epoch 2600/1000\n",
      "27.082977294921875\n",
      "0.11686486005783081\n",
      "Epoch 2650/1000\n",
      "29.732290267944336\n",
      "0.10703404247760773\n",
      "Epoch 2700/1000\n",
      "22.255212783813477\n",
      "0.10531452298164368\n",
      "Epoch 2750/1000\n",
      "35.6710319519043\n",
      "0.09528867900371552\n",
      "Epoch 2800/1000\n",
      "33.013179779052734\n",
      "0.09620936959981918\n",
      "Epoch 2850/1000\n",
      "31.51644515991211\n",
      "0.1048879623413086\n",
      "Epoch 2900/1000\n",
      "33.89342498779297\n",
      "0.09455674141645432\n",
      "Epoch 2950/1000\n"
     ]
    }
   ],
   "source": [
    "Q.eval()\n",
    "for epoch in range(3000):\n",
    "    for obs, obs_, params, actions, rewards in q_dataloader:\n",
    "        O = obs_norm(Variable(obs.cuda()))\n",
    "        Z = params_norm(Variable(params.cuda()))\n",
    "        policy.zero_grad()\n",
    "        U = policy(O, Z)\n",
    "        loss = -Q(O, Z, U).sum() + (U ** 2).sum()\n",
    "        loss.backward()\n",
    "        policy_opt.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(-Q(O, Z, U).sum().data[0])\n",
    "        print((U ** 2).sum().data[0])\n",
    "        print('Epoch {}/{}'.format(epoch, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-02 *\n",
       "  0.6123 -0.3149  0.0254  0.3780 -0.0079 -1.6828 -0.5863\n",
       "  3.9200  0.8016 -3.2435  0.3036 -0.4587 -0.7278  1.6921\n",
       " -0.2203  3.5576  0.4977 -1.8740 -0.5076  0.5001 -0.4833\n",
       " -0.9771 -4.3675  0.7175 -0.0042 -1.7637  1.1436  1.3261\n",
       "  2.6981 -0.7015 -2.4304 -1.4848 -2.6430  0.7728  4.3100\n",
       "  0.1475 -1.6741  1.2597 -0.5295  2.7992  0.2179 -5.7483\n",
       " -0.1156  2.1131 -2.0877 -0.6397  0.9042  0.4141 -1.5236\n",
       "  4.3233 -4.9378 -4.1355  3.4972  1.9722 -0.9067 -3.2555\n",
       "  0.3052 -4.9824 -0.4607  3.4206 -0.2770 -0.6339 -0.3770\n",
       "  2.5423 -3.6782 -2.6442  2.3909  0.1963 -0.4790 -0.5913\n",
       "  0.6066 -4.7768 -0.2425  1.6004  1.1045  0.7858 -2.4418\n",
       "  2.4646  2.8561 -1.6813 -2.7845 -2.0047  0.4530  2.8300\n",
       " -1.5845  2.6690  0.5944 -1.3046  0.7115 -1.7450 -2.5493\n",
       "  2.8614  2.0113 -3.1274 -0.1827  1.9345 -1.4098 -1.7068\n",
       "  1.1144  3.7867 -1.8957 -1.4210  1.4229 -1.5880 -0.8658\n",
       "  1.4810  2.9005 -1.4825  1.0102  0.9865 -2.0209 -2.6164\n",
       "  3.4437  1.0478 -2.2500 -1.0605  0.6287  0.1097  0.2071\n",
       "  1.9897  3.1723  0.0715 -3.3042  0.8326  0.3629 -1.4227\n",
       " -2.8022 -0.3505  2.2708 -0.8100 -1.4901  0.7580  1.0205\n",
       " -0.6406  2.2454  3.7144 -2.1227  1.3240  0.2214 -3.3552\n",
       "  4.0207  2.5494 -4.5501 -1.3582  3.1308 -0.9058 -2.5103\n",
       "  1.6825 -2.9507 -1.8763  2.3823 -1.9878 -1.4057  1.1220\n",
       "  3.0355  0.4518 -1.7864 -1.0368  0.3764  0.4936  0.2811\n",
       " -1.4277  4.7691  3.5255 -2.9624 -1.1528 -0.2762  1.3565\n",
       " -2.2082  2.7346  1.1049 -1.0647 -1.9380  0.1154  0.8210\n",
       "  0.4112 -2.0646 -1.1405  0.3733 -1.2387  0.4302  1.2000\n",
       " -0.4997  1.8808 -0.0681 -1.8372  1.3032  0.9555 -0.7779\n",
       "  1.5258 -0.0180  1.3500 -1.6060 -1.1995 -0.0323  1.8200\n",
       " -2.4115 -1.6874  3.0289 -0.4805 -2.2687  0.8610  0.4899\n",
       "  2.3008  2.6937 -3.1613 -1.8641  2.7282 -0.5015 -3.7105\n",
       "  1.5880  3.0807 -0.6973 -2.5820  0.2070  0.5530  0.0754\n",
       " -0.6469 -0.0030  2.2964 -1.9536  1.8804  0.6747 -3.0065\n",
       " -0.3326  1.4161  1.1262 -1.0386 -1.7522  0.1857  0.6231\n",
       " -0.4258 -1.5845  1.7129  0.7419  1.1922 -0.0059 -4.0837\n",
       " -2.6980 -0.9882  5.0690 -1.9940 -0.8816  0.9822 -2.0175\n",
       "  2.3016  2.8851 -1.9256 -1.0103  0.9040 -0.4796 -2.0221\n",
       "  3.2826 -4.4184 -3.0710  3.0707  0.9797 -0.7462 -1.2819\n",
       " -1.8938 -2.7482  2.6987  0.2422 -2.6326  0.2788  0.0785\n",
       "  0.6001 -3.1146  1.1819  1.6464  0.2141 -0.3554 -1.1701\n",
       "  7.0716 -0.9316 -5.9890  1.4161  0.7043 -1.3093  0.3665\n",
       "  1.5630 -3.6611 -2.0959  1.2126 -2.4436 -0.3889  2.7587\n",
       " -0.1281  1.7548  1.7460 -1.6183 -2.2833 -0.5680  0.5661\n",
       "  1.5160 -5.1093 -0.2858  2.6688 -2.0107 -0.8857  1.3279\n",
       "  0.6008 -4.8943 -0.8637  2.7183 -2.0477 -0.6226  1.7572\n",
       "  2.5490 -2.8877 -2.5834  0.5913  2.6195 -0.5647 -4.4601\n",
       " -0.6450 -0.5509 -1.0826  0.5491  1.8680  0.3136 -1.9115\n",
       "  2.6694 -2.7592 -3.8365  2.6343  2.6589 -0.8789 -2.1186\n",
       " -0.1618  0.1919  2.6852 -1.8154  1.2803  1.6521 -2.0955\n",
       " -0.5783 -3.8848  0.0878  1.6135 -2.5920 -0.3198  1.9124\n",
       "  1.3502  1.0141 -0.8917 -3.1904 -2.8627  0.8895  4.5288\n",
       " -1.4818 -1.3582  3.6879 -1.0471 -1.9227  0.5065 -0.8726\n",
       " -2.3191  2.7324  1.2036 -1.0250 -1.0475 -1.0391  1.2291\n",
       " -4.1521 -1.3021  4.7984 -0.2420 -0.7387  0.2710 -2.1097\n",
       " -1.4663 -0.1254  3.7293 -1.0850  2.8122  0.9891 -4.8511\n",
       " -2.0247 -0.1114  2.4812  0.8327  3.0806 -0.4553 -3.5739\n",
       "  4.9305 -2.7404 -3.9808  1.7519  0.2202 -1.0171  0.2521\n",
       " -0.7507  2.1795 -1.0825 -1.6369  0.0177  0.6175  0.5864\n",
       " -2.6758  2.3667  3.1906 -2.7379  0.9323  0.8025 -1.7936\n",
       " -2.4100  2.7933  4.0853 -2.8753 -1.6248 -0.1070 -0.9322\n",
       " -0.5063  2.6128 -0.2874 -1.1496  1.3180 -0.3735 -2.8986\n",
       "  2.4720 -3.6996 -1.9424  0.9357  1.1330  0.5377 -1.8446\n",
       "  1.7494  1.7948 -0.3576 -2.3225  1.0967  1.0507 -0.9383\n",
       " -1.2770  0.1707  1.4851 -1.4129 -1.6842 -0.3274  1.0076\n",
       " -2.2680  3.1156  3.9743 -2.4734  3.0112  0.9389 -3.6547\n",
       " -3.9108 -1.7748  4.0497 -0.2422 -0.9312 -0.0619 -1.6480\n",
       "  3.7907  0.3011 -3.0420 -0.9926 -0.0032  0.2739  1.4063\n",
       " -0.6517 -2.9587  1.7229  2.0823  0.0672 -0.7089 -3.0872\n",
       "  3.1258 -2.6509 -3.7272  0.3741  0.7510 -0.4462 -0.1520\n",
       " -0.5755 -1.1627  0.1118 -1.0799  1.5462 -0.1083 -3.8286\n",
       "  4.8201  0.1095 -1.9800  0.0347  4.4602  0.2102 -5.5720\n",
       "  0.8655  1.4896 -0.1453 -2.6259  0.4224  1.2761 -0.5401\n",
       " -3.8182  4.2967  2.2627 -3.3799  0.0175  0.3572 -0.9234\n",
       "  2.2935 -1.0146 -1.3917  0.1148 -3.3274 -0.9552  4.1236\n",
       " -1.0643 -4.9663  0.6196  2.4276 -2.4508 -0.5041 -0.2031\n",
       "  1.3260 -2.6162 -1.3063  1.0999 -2.6888 -0.3171  2.4876\n",
       " -2.6250  1.4556  3.0113 -2.1143 -1.1925 -0.4777 -0.5237\n",
       "  2.6200  1.0210  0.5078  0.4804  3.4036 -1.1350 -6.2813\n",
       "  2.6202 -0.3743 -1.2460  0.6897 -1.7017 -1.2643 -0.3254\n",
       "  2.0760 -2.9110 -1.4284 -0.1096  0.9705  0.8633 -1.7489\n",
       "  0.0522 -4.0068 -0.3306  2.3419 -0.2364 -0.5080 -1.8028\n",
       " -0.3718  0.5295 -0.1600  0.0290  2.7437 -0.0683 -5.7243\n",
       " -1.5471 -0.1050  3.7395 -1.1562  2.2358  0.9334 -4.3249\n",
       " -1.6937 -3.7069  1.3165 -0.4280 -2.0327  1.0249  1.3674\n",
       "  0.3878 -1.6803  2.5374 -0.5841  2.4624  0.4908 -5.4119\n",
       "  6.2447  0.1847 -4.1177  1.2295  2.4778 -1.7716 -4.0137\n",
       " -2.0801  0.7740  1.6533 -1.3598 -3.2679 -0.3389  2.7378\n",
       " -1.4634 -0.1018  2.4858  0.7013  0.3129 -0.3795 -1.0034\n",
       " -2.1849 -1.1233  0.3173  0.4916  0.5437  0.2458 -1.5359\n",
       " -2.8243  3.0279  4.7236 -2.9864 -2.3728  0.5046  0.3403\n",
       "  1.0664  1.1019 -0.3206 -0.1718 -1.2914 -0.5734  1.0876\n",
       " -1.2896 -1.2788 -1.2108  2.6246 -1.0783 -2.3763 -1.0627\n",
       "  0.3699  0.7335  2.2877 -2.2226  1.8759  0.9773 -2.3568\n",
       " -1.5360  0.4641  2.4349  0.2850  0.4268 -1.3876 -2.2177\n",
       "  3.6213  1.4944 -3.1162 -1.7225 -1.7417  0.0383  2.3884\n",
       " -1.5415  1.7772  2.4443 -3.1748 -2.7055  0.9249  1.7013\n",
       "  0.0373  3.3005 -1.3509 -0.8917 -0.8795  0.1224  0.1624\n",
       "  3.3378 -1.9329 -0.4729  2.0562  2.9523 -1.5442 -6.3207\n",
       " -0.5949  0.9804  3.1441 -1.6085  1.6415 -0.5328 -4.0989\n",
       " -0.5301  2.4570  0.4128 -3.1290 -0.0484 -0.1274 -1.3386\n",
       "  0.3336  2.7690  2.0425 -3.3757 -2.0016  0.4972  2.1002\n",
       " -2.2524  4.4185  3.0450 -3.0538 -1.6866  0.0529  2.1391\n",
       " -5.4312  3.4545  6.0031 -2.9599 -2.5572  0.1598  0.3436\n",
       "  1.8627 -2.5731 -2.2781  1.6635 -1.1874 -0.5882  1.4435\n",
       "  0.7417 -2.7449 -1.2749  1.3673 -0.2116 -1.4084 -2.0205\n",
       " -0.6862 -2.1422 -0.5066  2.9210  1.8021 -0.6699 -2.7091\n",
       "  1.2243  2.8127  0.7057 -4.0369  1.1845  0.6321 -0.5389\n",
       "  3.8713  0.7920 -0.6434  0.0115 -2.4606 -0.9929  2.2137\n",
       " -0.3658  1.0649 -0.5048 -0.8832  0.8011 -1.5032 -1.5195\n",
       " -0.2898  1.6384  0.2134 -1.6793 -3.6825 -0.9957  4.6493\n",
       " -2.3705 -0.3108  5.0231 -0.3197  0.8307  0.5532 -2.8148\n",
       "  5.0121  2.3452 -1.4965  0.0781 -0.2142 -1.8151 -1.3919\n",
       " -0.8021 -2.7655  1.8694  0.3173 -1.0255 -0.2858 -0.7188\n",
       " -0.3932  4.1896  2.9295 -2.6400 -1.7937 -1.5922 -1.4446\n",
       "  2.4853  4.2537 -2.2896 -2.4491  3.5457 -0.6430 -2.9830\n",
       " -1.2208  0.6939 -0.6573 -1.2527 -1.5790  0.6310  2.0509\n",
       " -1.3753  2.0545  2.4516 -1.8660 -1.0474  0.4538  0.4532\n",
       "  1.9896  0.1520 -0.9679 -0.9867 -0.3086  0.8300  0.7777\n",
       " -1.6036 -2.2450  2.0913  0.1978  1.7474  0.7265 -2.3505\n",
       "  0.4804  0.6505 -1.1559  1.3833  0.5811 -0.8808 -1.6356\n",
       "  2.8016 -1.4385 -2.3616 -0.0817 -0.3081 -1.0574 -1.6713\n",
       " -0.7757  1.4973  2.5916 -3.0547 -2.5321  0.6965  1.0873\n",
       "  3.2195 -3.1135 -1.2077  1.2559  1.0578 -1.0697 -3.8379\n",
       "  4.2043  0.6472 -1.8603  0.3727  1.9307 -1.2651 -4.0168\n",
       " -1.6182 -4.2429  1.4633  1.9519 -0.9730 -0.0222 -0.9108\n",
       "  0.3449 -0.5669  1.6944 -0.1922  1.4990 -1.6974 -4.7234\n",
       "  0.1737 -3.2366 -0.1020  2.3919 -2.3450 -1.3669 -0.0721\n",
       "  2.4948  3.0661 -0.8028 -1.4201  0.3767 -0.3913 -1.8193\n",
       "  1.2643 -4.3064 -1.2944  2.8485  1.3980 -1.0232 -4.4593\n",
       "[torch.cuda.FloatTensor of size 128x7 (GPU 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, obs_, params, actions, rewards = next(iter(q_dataloader))\n",
    "O = obs_norm(Variable(obs).cuda())\n",
    "Z = params_norm(Variable(params).cuda())\n",
    "policy(O, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('YumiReacher-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-749d11ca2d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export LD_LIBRARY_PATH=$HOME/.mujoco/mjpro150/bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DISPLAY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#export LD_LIBRARY_PATH=$HOME/.mujoco/mjpro150/bin\n",
    "#export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so\n",
    "os.environ.putenv('DISPLAY', ':0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy.eval()\n",
    "o = env.reset()\n",
    "rs = []\n",
    "for _ in range(1000):\n",
    "    O = obs_norm(Variable(torch.FloatTensor([o])).cuda())\n",
    "    Z = params_norm(Variable(torch.FloatTensor([[0.0, 0.0, 0.2, 0.9]])).cuda())\n",
    "    U = policy(O, Z)\n",
    "    u = U.cpu().data.numpy().flatten()\n",
    "    o, r, done, info = env.step(u)\n",
    "    rs.append(r)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Timestep')\n",
    "plt.plot(rs)\n",
    "plt.savefig('/home/isacar/workspace/rl-paper/rollout-rewards.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
